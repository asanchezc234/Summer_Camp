{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7296da94",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction - Making High-Dimensional Data Understandable\n",
    "\n",
    "This notebook will teach you how to reduce the complexity of your data whilst preserving the most important information.\n",
    "\n",
    "**What we'll learn:**\n",
    "1. Understanding the curse of dimensionality\n",
    "2. Principal Component Analysis (PCA) - finding the main patterns\n",
    "3. UMAP (Uniform Manifold Approximation) - preserving local structure (in theory)\n",
    "4. When to use each technique\n",
    "5. Visualising high-dimensional data in 2D/3D\n",
    "6. Practical applications\n",
    "\n",
    "**What is dimensionality reduction?**\n",
    "Imagine you have a dataset with 100 features, but only 3-4 of them really matter for understanding your data. Dimensionality reduction finds the features, or combination of features, that capture the most important information!\n",
    "\n",
    "**Prerequisites:**\n",
    "- Complete the clustering analysis notebook first\n",
    "- Have your clean data ready\n",
    "- Basic understanding of data preprocessing\n",
    "\n",
    "**Instructions:**\n",
    "- Run each cell in order\n",
    "- Try different numbers of components\n",
    "- Compare PCA and UMAP results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a986674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP imported successfully!\n",
      "Libraries imported successfully!\n",
      "Ready to explore dimensionality reduction!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries for dimensionality reduction and visualisation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For UMAP, we need to install it first if not available\n",
    "try:\n",
    "    import umap.umap_ as umap\n",
    "    print(\"UMAP imported successfully!\")\n",
    "except ImportError:\n",
    "    print(\"Installing UMAP...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"umap-learn\"])\n",
    "    import umap.umap_ as umap\n",
    "    print(\"UMAP installed and imported successfully!\")\n",
    "\n",
    "# Set up nice-looking plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Ready to explore dimensionality reduction!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9007859",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Your Data\n",
    "\n",
    "Let's start by loading our clean data and understanding its dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a296b926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully!\n",
      "Shape: 64620 rows √ó 11 columns\n",
      "\n",
      "üìä Dataset Overview:\n",
      "- Original dimensions: 11 features\n",
      "- Sample size: 64620 observations\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>work_year</th>\n",
       "      <th>experience_level</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary</th>\n",
       "      <th>salary_currency</th>\n",
       "      <th>salary_in_usd</th>\n",
       "      <th>employee_residence</th>\n",
       "      <th>remote_ratio</th>\n",
       "      <th>company_location</th>\n",
       "      <th>company_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025</td>\n",
       "      <td>SE</td>\n",
       "      <td>FT</td>\n",
       "      <td>Solutions Engineer</td>\n",
       "      <td>214000</td>\n",
       "      <td>USD</td>\n",
       "      <td>214000</td>\n",
       "      <td>US</td>\n",
       "      <td>100</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>158800</td>\n",
       "      <td>USD</td>\n",
       "      <td>158800</td>\n",
       "      <td>AU</td>\n",
       "      <td>0</td>\n",
       "      <td>AU</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025</td>\n",
       "      <td>MI</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>139200</td>\n",
       "      <td>USD</td>\n",
       "      <td>139200</td>\n",
       "      <td>AU</td>\n",
       "      <td>0</td>\n",
       "      <td>AU</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025</td>\n",
       "      <td>EN</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>90000</td>\n",
       "      <td>USD</td>\n",
       "      <td>90000</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025</td>\n",
       "      <td>EN</td>\n",
       "      <td>FT</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>80000</td>\n",
       "      <td>USD</td>\n",
       "      <td>80000</td>\n",
       "      <td>US</td>\n",
       "      <td>0</td>\n",
       "      <td>US</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   work_year experience_level employment_type           job_title  salary  \\\n",
       "0       2025               SE              FT  Solutions Engineer  214000   \n",
       "1       2025               MI              FT       Data Engineer  158800   \n",
       "2       2025               MI              FT       Data Engineer  139200   \n",
       "3       2025               EN              FT       Data Engineer   90000   \n",
       "4       2025               EN              FT       Data Engineer   80000   \n",
       "\n",
       "  salary_currency  salary_in_usd employee_residence  remote_ratio  \\\n",
       "0             USD         214000                 US           100   \n",
       "1             USD         158800                 AU             0   \n",
       "2             USD         139200                 AU             0   \n",
       "3             USD          90000                 US             0   \n",
       "4             USD          80000                 US             0   \n",
       "\n",
       "  company_location company_size  \n",
       "0               US            M  \n",
       "1               AU            M  \n",
       "2               AU            M  \n",
       "3               US            M  \n",
       "4               US            M  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your clean data\n",
    "df = pd.read_csv('../datasets/cleaned_data.csv')\n",
    "\n",
    "print(f\"Data loaded successfully!\")\n",
    "print(f\"Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "\n",
    "# Show basic information\n",
    "print(f\"\\nüìä Dataset Overview:\")\n",
    "print(f\"- Original dimensions: {df.shape[1]} features\")\n",
    "print(f\"- Sample size: {df.shape[0]} observations\")\n",
    "\n",
    "# Look at the data\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a03fad20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç FEATURE ANALYSIS\n",
      "==================================================\n",
      "üìä Numerical features (4):\n",
      "1. work_year (variance: 0.5)\n",
      "2. salary (variance: 88940176661.5)\n",
      "3. salary_in_usd (variance: 6010943493.5)\n",
      "4. remote_ratio (variance: 1844.4)\n",
      "\n",
      "üìù Categorical features (7):\n",
      "1. experience_level (4 categories)\n",
      "2. employment_type (4 categories)\n",
      "3. job_title (390 categories)\n",
      "4. salary_currency (26 categories)\n",
      "5. employee_residence (102 categories)\n",
      "6. company_location (95 categories)\n",
      "7. company_size (3 categories)\n",
      "\n",
      "üí° Why dimensionality reduction matters:\n",
      "- With 11 features, visualisation is impossible\n",
      "- Some features might be redundant or correlated\n",
      "- Reducing dimensions makes data easier to understand and plot\n",
      "- Can improve machine learning model performance\n"
     ]
    }
   ],
   "source": [
    "# Examine our features\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"üîç FEATURE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"üìä Numerical features ({len(numerical_features)}):\")\n",
    "for i, feature in enumerate(numerical_features, 1):\n",
    "    variance = df[feature].var()\n",
    "    print(f\"{i}. {feature} (variance: {variance:.1f})\")\n",
    "\n",
    "print(f\"\\nüìù Categorical features ({len(categorical_features)}):\")\n",
    "for i, feature in enumerate(categorical_features, 1):\n",
    "    n_categories = df[feature].nunique()\n",
    "    print(f\"{i}. {feature} ({n_categories} categories)\")\n",
    "\n",
    "print(f\"\\nüí° Why dimensionality reduction matters:\")\n",
    "print(f\"- With {df.shape[1]} features, visualisation is impossible\")\n",
    "print(f\"- Some features might be redundant or correlated\")\n",
    "print(f\"- Reducing dimensions makes data easier to understand and plot\")\n",
    "print(f\"- Can improve machine learning model performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28455355",
   "metadata": {},
   "source": [
    "## 2. Prepare Features for Dimensionality Reduction\n",
    "\n",
    "We need to create a comprehensive feature matrix, similar to what we did for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd2b3b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß CREATING COMPREHENSIVE FEATURE MATRIX\n",
      "==================================================\n",
      "‚úì Added 4 numerical features\n",
      "\n",
      "üî§ Encoding categorical features...\n",
      "  Including experience_level: 4 categories\n",
      "  Including employment_type: 4 categories\n",
      "  Skipping job_title: 390 categories (too many)\n",
      "  Skipping salary_currency: 26 categories (too many)\n",
      "  Skipping employee_residence: 102 categories (too many)\n",
      "  Skipping company_location: 95 categories (too many)\n",
      "  Including company_size: 3 categories\n",
      "‚úì Encoded 3 categorical features\n",
      "  Created 8 binary features\n",
      "\n",
      "‚úÖ COMPLETE FEATURE MATRIX:\n",
      "   Shape: (64620, 12)\n",
      "   Features: ['work_year', 'salary', 'salary_in_usd', 'remote_ratio', 'experience_level_EX']... (showing first 5)\n"
     ]
    }
   ],
   "source": [
    "# Create feature matrix with both numerical and categorical features\n",
    "print(\"üîß CREATING COMPREHENSIVE FEATURE MATRIX\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "feature_frames = []\n",
    "\n",
    "# Add all numerical features\n",
    "if len(numerical_features) > 0:\n",
    "    X_numerical = df[numerical_features].copy()\n",
    "    feature_frames.append(X_numerical)\n",
    "    print(f\"‚úì Added {len(numerical_features)} numerical features\")\n",
    "\n",
    "# Encode categorical features (limit to reasonable number of categories)\n",
    "categorical_for_encoding = []\n",
    "if len(categorical_features) > 0:\n",
    "    print(f\"\\nüî§ Encoding categorical features...\")\n",
    "    \n",
    "    for feature in categorical_features:\n",
    "        n_categories = df[feature].nunique()\n",
    "        if n_categories <= 20:  # Only encode features with reasonable categories\n",
    "            categorical_for_encoding.append(feature)\n",
    "            print(f\"  Including {feature}: {n_categories} categories\")\n",
    "        else:\n",
    "            print(f\"  Skipping {feature}: {n_categories} categories (too many)\")\n",
    "    \n",
    "    if categorical_for_encoding:\n",
    "        # One-hot encode selected categorical features\n",
    "        categorical_encoded_frames = []\n",
    "        for feature in categorical_for_encoding:\n",
    "            encoded = pd.get_dummies(df[feature], prefix=feature, drop_first=True)\n",
    "            categorical_encoded_frames.append(encoded)\n",
    "        \n",
    "        X_categorical = pd.concat(categorical_encoded_frames, axis=1)\n",
    "        feature_frames.append(X_categorical)\n",
    "        print(f\"‚úì Encoded {len(categorical_for_encoding)} categorical features\")\n",
    "        print(f\"  Created {X_categorical.shape[1]} binary features\")\n",
    "\n",
    "# Combine all features\n",
    "X = pd.concat(feature_frames, axis=1)\n",
    "print(f\"\\n‚úÖ COMPLETE FEATURE MATRIX:\")\n",
    "print(f\"   Shape: {X.shape}\")\n",
    "print(f\"   Features: {list(X.columns)[:5]}... (showing first 5)\")\n",
    "\n",
    "# Check for any missing values\n",
    "if X.isnull().sum().sum() > 0:\n",
    "    print(f\"‚ö†Ô∏è  Found missing values, filling with median/mode\")\n",
    "    # Fill numerical columns with median\n",
    "    for col in X.select_dtypes(include=[np.number]).columns:\n",
    "        X[col] = X[col].fillna(X[col].median())\n",
    "    # Fill categorical columns with mode\n",
    "    for col in X.select_dtypes(include=['object']).columns:\n",
    "        X[col] = X[col].fillna(X[col].mode().iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584efb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è STANDARDISING FEATURES\n",
      "==================================================\n",
      "‚úì Features standardised!\n",
      "All features now have mean ‚âà 0 and standard deviation ‚âà 1\n",
      "\n",
      "Before standardisation:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/sc2024/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:218\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 218\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sc2024/lib/python3.11/site-packages/pandas/core/computation/expressions.py:242\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_numexpr:\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;66;03m# error: \"None\" not callable\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sc2024/lib/python3.11/site-packages/pandas/core/computation/expressions.py:73\u001b[0m, in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m     72\u001b[0m     _store_test_result(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Show the standardisation effect\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBefore standardisation:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Feature ranges: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mdescribe()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAfter standardisation:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Feature ranges: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(X_scaled_df\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mX_scaled_df\u001b[38;5;241m.\u001b[39mmin())\u001b[38;5;241m.\u001b[39mdescribe()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sc2024/lib/python3.11/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sc2024/lib/python3.11/site-packages/pandas/core/arraylike.py:194\u001b[0m, in \u001b[0;36mOpsMixin.__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sub__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sc2024/lib/python3.11/site-packages/pandas/core/series.py:6135\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[1;32m   6134\u001b[0m     \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_for_op(other)\n\u001b[0;32m-> 6135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexOpsMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sc2024/lib/python3.11/site-packages/pandas/core/base.py:1382\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1379\u001b[0m     rvalues \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(rvalues\u001b[38;5;241m.\u001b[39mstart, rvalues\u001b[38;5;241m.\u001b[39mstop, rvalues\u001b[38;5;241m.\u001b[39mstep)\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1382\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(result, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sc2024/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:283\u001b[0m, in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    279\u001b[0m     _bool_arith_check(op, left, right)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;66;03m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[39;00m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43m_na_arithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sc2024/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:227\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    221\u001b[0m         left\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(right, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[1;32m    222\u001b[0m     ):\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43m_masked_arith_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/sc2024/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:163\u001b[0m, in \u001b[0;36m_masked_arith_op\u001b[0;34m(x, y, op)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# See GH#5284, GH#5035, GH#19448 for historical reference\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m--> 163\u001b[0m         result[mask] \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxrav\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myrav\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(y):\n",
      "\u001b[0;31mTypeError\u001b[0m: numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead."
     ]
    }
   ],
   "source": [
    "# Standardise features (essential for dimensionality reduction!)\n",
    "print(\"‚öñÔ∏è STANDARDISING FEATURES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "\n",
    "print(\"‚úì Features standardised!\")\n",
    "print(f\"All features now have mean ‚âà 0 and standard deviation ‚âà 1\")\n",
    "\n",
    "# Show the standardisation effect\n",
    "print(f\"\\nBefore standardisation:\")\n",
    "# Only show ranges for numerical features to avoid boolean arithmetic errors\n",
    "numerical_columns = X.select_dtypes(include=[np.number]).columns\n",
    "if len(numerical_columns) > 0:\n",
    "    numerical_ranges = X[numerical_columns].max() - X[numerical_columns].min()\n",
    "    print(f\"  Numerical feature ranges: {numerical_ranges.describe()}\")\n",
    "else:\n",
    "    print(f\"  No numerical features to show ranges for\")\n",
    "\n",
    "print(f\"\\nAfter standardisation:\")\n",
    "standardised_ranges = X_scaled_df.max() - X_scaled_df.min()\n",
    "print(f\"  All feature ranges: {standardised_ranges.describe()}\")\n",
    "\n",
    "print(f\"\\nüîç Why standardisation is crucial:\")\n",
    "print(f\"- PCA is sensitive to feature scales\")\n",
    "print(f\"- UMAP also benefits from standardised features\")\n",
    "print(f\"- Prevents features with large values from dominating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd93fe6",
   "metadata": {},
   "source": [
    "## 3. Principal Component Analysis (PCA)\n",
    "\n",
    "PCA finds the directions of maximum variance in your data. Think of it as finding the \"best camera angles\" to view your data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302e4e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to find the optimal number of components\n",
    "print(\"üîç ANALYSING PRINCIPAL COMPONENTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Fit PCA with all possible components\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_scaled)\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "print(f\"Total features: {X_scaled.shape[1]}\")\n",
    "print(f\"Total variance explained by all components: {cumulative_variance[-1]:.3f}\")\n",
    "\n",
    "# Find how many components explain 90% and 95% of variance\n",
    "components_90 = np.argmax(cumulative_variance >= 0.90) + 1\n",
    "components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"\\nüìä Variance explanation:\")\n",
    "print(f\"- {components_90} components explain 90% of variance\")\n",
    "print(f\"- {components_95} components explain 95% of variance\")\n",
    "print(f\"- This means we can reduce from {X_scaled.shape[1]} to {components_90} dimensions!\")\n",
    "\n",
    "# Show top components\n",
    "print(f\"\\nTop 10 components and their variance:\")\n",
    "for i in range(min(10, len(pca_full.explained_variance_ratio_))):\n",
    "    print(f\"PC{i+1}: {pca_full.explained_variance_ratio_[i]:.3f} ({pca_full.explained_variance_ratio_[i]*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae619b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the explained variance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Individual explained variance\n",
    "ax1.bar(range(1, min(21, len(pca_full.explained_variance_ratio_) + 1)), \n",
    "        pca_full.explained_variance_ratio_[:20])\n",
    "ax1.set_xlabel('Principal Component')\n",
    "ax1.set_ylabel('Explained Variance Ratio')\n",
    "ax1.set_title('Individual Explained Variance (First 20 Components)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative explained variance\n",
    "ax2.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'bo-', markersize=4)\n",
    "ax2.axhline(y=0.90, color='r', linestyle='--', alpha=0.7, label='90% threshold')\n",
    "ax2.axhline(y=0.95, color='orange', linestyle='--', alpha=0.7, label='95% threshold')\n",
    "ax2.axvline(x=components_90, color='r', linestyle=':', alpha=0.7)\n",
    "ax2.axvline(x=components_95, color='orange', linestyle=':', alpha=0.7)\n",
    "ax2.set_xlabel('Number of Components')\n",
    "ax2.set_ylabel('Cumulative Explained Variance')\n",
    "ax2.set_title('Cumulative Explained Variance')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"How to interpret these plots:\")\n",
    "print(\"‚Ä¢ Left: How much each component contributes individually\")\n",
    "print(\"‚Ä¢ Right: How much variance is captured as we add more components\")\n",
    "print(\"‚Ä¢ The 'elbow' in the right plot suggests optimal number of components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6006bbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA for 2D and 3D visualisation\n",
    "print(\"üé® CREATING 2D AND 3D VISUALISATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 2D PCA\n",
    "pca_2d = PCA(n_components=2, random_state=42)\n",
    "X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "# 3D PCA  \n",
    "pca_3d = PCA(n_components=3, random_state=42)\n",
    "X_pca_3d = pca_3d.fit_transform(X_scaled)\n",
    "\n",
    "# Also create a reduced dataset with optimal components\n",
    "pca_optimal = PCA(n_components=components_90, random_state=42)\n",
    "X_pca_optimal = pca_optimal.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"‚úì 2D PCA: {X_scaled.shape[1]} ‚Üí 2 dimensions\")\n",
    "print(f\"  Variance explained: {pca_2d.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "print(f\"‚úì 3D PCA: {X_scaled.shape[1]} ‚Üí 3 dimensions\") \n",
    "print(f\"  Variance explained: {pca_3d.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "print(f\"‚úì Optimal PCA: {X_scaled.shape[1]} ‚Üí {components_90} dimensions\")\n",
    "print(f\"  Variance explained: {pca_optimal.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "# Store PCA results in DataFrame for easy plotting\n",
    "pca_results_df = pd.DataFrame({\n",
    "    'PC1': X_pca_2d[:, 0],\n",
    "    'PC2': X_pca_2d[:, 1],\n",
    "    'PC3': X_pca_3d[:, 2] if X_pca_3d.shape[1] > 2 else np.zeros(len(X_pca_2d))\n",
    "})\n",
    "\n",
    "print(f\"\\nüìä PCA Results Summary:\")\n",
    "print(f\"- Original dimensions: {X_scaled.shape[1]}\")\n",
    "print(f\"- 2D captures: {pca_2d.explained_variance_ratio_.sum()*100:.1f}% of variance\")\n",
    "print(f\"- 3D captures: {pca_3d.explained_variance_ratio_.sum()*100:.1f}% of variance\") \n",
    "print(f\"- {components_90}D captures: {pca_optimal.explained_variance_ratio_.sum()*100:.1f}% of variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d27dffe",
   "metadata": {},
   "source": [
    "## 4. UMAP (Uniform Manifold Approximation and Projection)\n",
    "\n",
    "UMAP preserves local structure better than PCA. It's excellent for visualising complex, non-linear relationships!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bebff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply UMAP for 2D and 3D visualisation\n",
    "print(\"üó∫Ô∏è APPLYING UMAP DIMENSIONALITY REDUCTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# For large datasets, we might want to sample for faster computation\n",
    "sample_size = min(10000, len(X_scaled))  # Use up to 10,000 points\n",
    "if len(X_scaled) > sample_size:\n",
    "    print(f\"Using a sample of {sample_size} points for UMAP (from {len(X_scaled)} total)\")\n",
    "    sample_indices = np.random.choice(len(X_scaled), sample_size, replace=False)\n",
    "    X_sample = X_scaled[sample_indices]\n",
    "    df_sample = df.iloc[sample_indices].reset_index(drop=True)\n",
    "else:\n",
    "    X_sample = X_scaled\n",
    "    df_sample = df.copy()\n",
    "    sample_indices = np.arange(len(X_scaled))\n",
    "\n",
    "print(f\"Running UMAP on {len(X_sample)} data points...\")\n",
    "\n",
    "# 2D UMAP\n",
    "print(\"Computing 2D UMAP...\")\n",
    "umap_2d = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "X_umap_2d = umap_2d.fit_transform(X_sample)\n",
    "\n",
    "# 3D UMAP\n",
    "print(\"Computing 3D UMAP...\")\n",
    "umap_3d = umap.UMAP(n_components=3, random_state=42, n_neighbors=15, min_dist=0.1)\n",
    "X_umap_3d = umap_3d.fit_transform(X_sample)\n",
    "\n",
    "print(\"‚úì UMAP computations complete!\")\n",
    "\n",
    "# Store UMAP results\n",
    "umap_results_df = pd.DataFrame({\n",
    "    'UMAP1': X_umap_2d[:, 0],\n",
    "    'UMAP2': X_umap_2d[:, 1],\n",
    "    'UMAP3': X_umap_3d[:, 2]\n",
    "})\n",
    "\n",
    "print(f\"\\nüìä UMAP Results Summary:\")\n",
    "print(f\"- Processed {len(X_sample)} data points\")\n",
    "print(f\"- 2D UMAP coordinates range: [{X_umap_2d.min():.1f}, {X_umap_2d.max():.1f}]\")\n",
    "print(f\"- 3D UMAP coordinates range: [{X_umap_3d.min():.1f}, {X_umap_3d.max():.1f}]\")\n",
    "\n",
    "print(f\"\\nüîç UMAP vs PCA:\")\n",
    "print(f\"- PCA: Linear transformation, preserves global structure\")\n",
    "print(f\"- UMAP: Non-linear transformation, preserves local structure\")\n",
    "print(f\"- UMAP often reveals clusters and local patterns better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac23530",
   "metadata": {},
   "source": [
    "## 5. Compare PCA and UMAP Visualisations\n",
    "\n",
    "Let's see how PCA and UMAP represent our data differently!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e6d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# PCA 2D (using sample if needed)\n",
    "pca_sample_indices = sample_indices if len(X_scaled) > sample_size else np.arange(len(X_scaled))\n",
    "X_pca_sample = X_pca_2d[pca_sample_indices] if len(X_scaled) > sample_size else X_pca_2d\n",
    "\n",
    "scatter1 = axes[0, 0].scatter(X_pca_sample[:, 0], X_pca_sample[:, 1], \n",
    "                              alpha=0.6, s=20, c='steelblue')\n",
    "axes[0, 0].set_xlabel('First Principal Component')\n",
    "axes[0, 0].set_ylabel('Second Principal Component')\n",
    "axes[0, 0].set_title('PCA 2D Projection')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# UMAP 2D\n",
    "scatter2 = axes[0, 1].scatter(X_umap_2d[:, 0], X_umap_2d[:, 1], \n",
    "                              alpha=0.6, s=20, c='coral')\n",
    "axes[0, 1].set_xlabel('UMAP Dimension 1')\n",
    "axes[0, 1].set_ylabel('UMAP Dimension 2')\n",
    "axes[0, 1].set_title('UMAP 2D Projection')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# If we have categorical features, colour by the first one\n",
    "if len(categorical_features) > 0:\n",
    "    color_feature = categorical_features[0]\n",
    "    \n",
    "    # Get unique categories and assign colours\n",
    "    unique_categories = df_sample[color_feature].unique()\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_categories)))\n",
    "    color_map = dict(zip(unique_categories, colors))\n",
    "    \n",
    "    # Create colour array\n",
    "    point_colors = [color_map[cat] for cat in df_sample[color_feature]]\n",
    "    \n",
    "    # PCA with categories\n",
    "    scatter3 = axes[1, 0].scatter(X_pca_sample[:, 0], X_pca_sample[:, 1], \n",
    "                                  c=point_colors, alpha=0.6, s=20)\n",
    "    axes[1, 0].set_xlabel('First Principal Component')\n",
    "    axes[1, 0].set_ylabel('Second Principal Component')\n",
    "    axes[1, 0].set_title(f'PCA 2D (coloured by {color_feature})')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # UMAP with categories\n",
    "    scatter4 = axes[1, 1].scatter(X_umap_2d[:, 0], X_umap_2d[:, 1], \n",
    "                                  c=point_colors, alpha=0.6, s=20)\n",
    "    axes[1, 1].set_xlabel('UMAP Dimension 1')\n",
    "    axes[1, 1].set_ylabel('UMAP Dimension 2')\n",
    "    axes[1, 1].set_title(f'UMAP 2D (coloured by {color_feature})')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                  markerfacecolor=color_map[cat], markersize=8, label=cat)\n",
    "                       for cat in unique_categories[:10]]  # Show first 10 categories\n",
    "    axes[1, 1].legend(handles=legend_elements, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "else:\n",
    "    # If no categorical features, show density plots\n",
    "    axes[1, 0].hexbin(X_pca_sample[:, 0], X_pca_sample[:, 1], gridsize=30, cmap='Blues')\n",
    "    axes[1, 0].set_xlabel('First Principal Component')\n",
    "    axes[1, 0].set_ylabel('Second Principal Component')\n",
    "    axes[1, 0].set_title('PCA 2D (density)')\n",
    "    \n",
    "    axes[1, 1].hexbin(X_umap_2d[:, 0], X_umap_2d[:, 1], gridsize=30, cmap='Reds')\n",
    "    axes[1, 1].set_xlabel('UMAP Dimension 1')\n",
    "    axes[1, 1].set_ylabel('UMAP Dimension 2')\n",
    "    axes[1, 1].set_title('UMAP 2D (density)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üé® Visualisation Comparison:\")\n",
    "print(\"‚Ä¢ PCA (blue): Shows global structure, linear relationships\")\n",
    "print(\"‚Ä¢ UMAP (coral): Shows local structure, non-linear relationships\")\n",
    "print(\"‚Ä¢ Look for clusters, patterns, and separations\")\n",
    "print(\"‚Ä¢ UMAP often reveals more detailed local structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb163d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D visualisations\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "\n",
    "# 3D PCA\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "X_pca_3d_sample = X_pca_3d[pca_sample_indices] if len(X_scaled) > sample_size else X_pca_3d\n",
    "\n",
    "scatter1 = ax1.scatter(X_pca_3d_sample[:, 0], X_pca_3d_sample[:, 1], X_pca_3d_sample[:, 2], \n",
    "                       alpha=0.6, s=20, c='steelblue')\n",
    "ax1.set_xlabel('PC1')\n",
    "ax1.set_ylabel('PC2')\n",
    "ax1.set_zlabel('PC3')\n",
    "ax1.set_title('PCA 3D Projection')\n",
    "\n",
    "# 3D UMAP\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "scatter2 = ax2.scatter(X_umap_3d[:, 0], X_umap_3d[:, 1], X_umap_3d[:, 2], \n",
    "                       alpha=0.6, s=20, c='coral')\n",
    "ax2.set_xlabel('UMAP1')\n",
    "ax2.set_ylabel('UMAP2')\n",
    "ax2.set_zlabel('UMAP3')\n",
    "ax2.set_title('UMAP 3D Projection')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üåê 3D Visualisation Benefits:\")\n",
    "print(\"‚Ä¢ Shows more complex relationships than 2D\")\n",
    "print(\"‚Ä¢ Can reveal layered structures in data\")\n",
    "print(\"‚Ä¢ Interactive rotation helps explore different angles\")\n",
    "print(\"‚Ä¢ 3D UMAP often shows clearer cluster separation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c6872c",
   "metadata": {},
   "source": [
    "## 6. Analyse the Components\n",
    "\n",
    "Let's understand what the principal components actually represent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676d7f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse PCA components\n",
    "print(\"üîç ANALYSING PRINCIPAL COMPONENTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get the loadings (how much each original feature contributes to each PC)\n",
    "loadings = pca_2d.components_.T * np.sqrt(pca_2d.explained_variance_)\n",
    "loadings_df = pd.DataFrame(loadings, \n",
    "                          columns=['PC1', 'PC2'], \n",
    "                          index=X.columns)\n",
    "\n",
    "print(\"Top contributors to each principal component:\")\n",
    "print(\"\\nüìä PC1 (explains {:.1f}% of variance):\".format(pca_2d.explained_variance_ratio_[0]*100))\n",
    "pc1_contributors = loadings_df['PC1'].abs().sort_values(ascending=False)\n",
    "for i, (feature, loading) in enumerate(pc1_contributors.head(5).items()):\n",
    "    print(f\"{i+1}. {feature}: {loading:.3f}\")\n",
    "\n",
    "print(\"\\nüìä PC2 (explains {:.1f}% of variance):\".format(pca_2d.explained_variance_ratio_[1]*100))\n",
    "pc2_contributors = loadings_df['PC2'].abs().sort_values(ascending=False)\n",
    "for i, (feature, loading) in enumerate(pc2_contributors.head(5).items()):\n",
    "    print(f\"{i+1}. {feature}: {loading:.3f}\")\n",
    "\n",
    "# Create a biplot to show both data points and feature vectors\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot the transformed data points\n",
    "scatter = ax.scatter(X_pca_sample[:, 0], X_pca_sample[:, 1], alpha=0.6, s=20)\n",
    "\n",
    "# Plot feature vectors (scaled for visibility)\n",
    "scale_factor = 3\n",
    "for i, (feature, row) in enumerate(loadings_df.iterrows()):\n",
    "    if i < 10:  # Show only first 10 features to avoid clutter\n",
    "        ax.arrow(0, 0, row['PC1']*scale_factor, row['PC2']*scale_factor, \n",
    "                head_width=0.1, head_length=0.1, fc='red', ec='red', alpha=0.7)\n",
    "        ax.text(row['PC1']*scale_factor*1.1, row['PC2']*scale_factor*1.1, \n",
    "                feature, fontsize=8, ha='center', va='center')\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}% variance)')\n",
    "ax.set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}% variance)')\n",
    "ax.set_title('PCA Biplot: Data Points + Feature Vectors')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "ax.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Understanding the Biplot:\")\n",
    "print(\"‚Ä¢ Points show your data in the new 2D space\")\n",
    "print(\"‚Ä¢ Red arrows show how original features relate to PCs\")\n",
    "print(\"‚Ä¢ Longer arrows = more important features\")\n",
    "print(\"‚Ä¢ Arrows pointing same direction = correlated features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a795481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create heatmap of top features for first few PCs\n",
    "n_components_show = min(5, pca_optimal.components_.shape[0])\n",
    "top_features_per_pc = 10\n",
    "\n",
    "# Get top features for each PC\n",
    "important_features = set()\n",
    "for i in range(n_components_show):\n",
    "    pc_loadings = np.abs(pca_optimal.components_[i])\n",
    "    top_indices = np.argsort(pc_loadings)[-top_features_per_pc:]\n",
    "    important_features.update([X.columns[j] for j in top_indices])\n",
    "\n",
    "important_features = list(important_features)\n",
    "\n",
    "# Create subset of loadings for heatmap\n",
    "loadings_subset = pca_optimal.components_[:n_components_show, :]\n",
    "loadings_subset_df = pd.DataFrame(loadings_subset.T, \n",
    "                                 index=X.columns,\n",
    "                                 columns=[f'PC{i+1}' for i in range(n_components_show)])\n",
    "\n",
    "# Filter to important features\n",
    "loadings_heatmap = loadings_subset_df.loc[important_features]\n",
    "\n",
    "sns.heatmap(loadings_heatmap.T, cmap='RdBu_r', center=0, \n",
    "            annot=False, fmt='.2f', cbar_kws={'label': 'Loading'})\n",
    "plt.title('Feature Loadings for Principal Components')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Principal Components')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üî• Feature Loadings Heatmap:\")\n",
    "print(\"‚Ä¢ Red = positive loading (feature increases with PC)\")\n",
    "print(\"‚Ä¢ Blue = negative loading (feature decreases with PC)\")\n",
    "print(\"‚Ä¢ White = neutral (feature doesn't contribute much)\")\n",
    "print(\"‚Ä¢ This shows which features define each principal component\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582269df",
   "metadata": {},
   "source": [
    "## 7. Practical Applications\n",
    "\n",
    "Let's explore how to use dimensionality reduction in real-world scenarios!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80406941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dimensionality reduction results\n",
    "print(\"üíæ SAVING DIMENSIONALITY REDUCTION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create comprehensive results dataset\n",
    "results_df = df.copy()\n",
    "\n",
    "# Add PCA results (for all data points)\n",
    "results_df['PCA_1'] = X_pca_2d[:, 0]\n",
    "results_df['PCA_2'] = X_pca_2d[:, 1]\n",
    "\n",
    "# Add UMAP results (map back to original indices if we used sampling)\n",
    "if len(X_scaled) > sample_size:\n",
    "    # Create full arrays filled with NaN\n",
    "    umap_1_full = np.full(len(df), np.nan)\n",
    "    umap_2_full = np.full(len(df), np.nan)\n",
    "    \n",
    "    # Fill in the sampled values\n",
    "    umap_1_full[sample_indices] = X_umap_2d[:, 0]\n",
    "    umap_2_full[sample_indices] = X_umap_2d[:, 1]\n",
    "    \n",
    "    results_df['UMAP_1'] = umap_1_full\n",
    "    results_df['UMAP_2'] = umap_2_full\n",
    "    print(f\"‚ö†Ô∏è  UMAP results only available for {len(sample_indices)} sampled points\")\n",
    "else:\n",
    "    results_df['UMAP_1'] = X_umap_2d[:, 0]\n",
    "    results_df['UMAP_2'] = X_umap_2d[:, 1]\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('dimensionality_reduction_results.csv', index=False)\n",
    "print(\"‚úì Results saved to 'dimensionality_reduction_results.csv'\")\n",
    "\n",
    "print(f\"\\nüìã Dataset now includes:\")\n",
    "print(f\"- Original features: {len(df.columns)} columns\")\n",
    "print(f\"- PCA coordinates: 2 columns\") \n",
    "print(f\"- UMAP coordinates: 2 columns\")\n",
    "print(f\"- Total: {len(results_df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8684f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate practical applications\n",
    "print(\"üí° PRACTICAL APPLICATIONS OF DIMENSIONALITY REDUCTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"1. üìä Data Visualisation:\")\n",
    "print(\"   - Impossible to plot 50+ dimensional data\")\n",
    "print(\"   - PCA/UMAP reduce to 2D/3D for plotting\")\n",
    "print(\"   - Reveals patterns invisible in high dimensions\")\n",
    "\n",
    "print(\"\\n2. üöÄ Machine Learning Preprocessing:\")\n",
    "print(\"   - Reduces computational cost\")\n",
    "print(\"   - Can improve model performance\")\n",
    "print(\"   - Removes noisy/redundant features\")\n",
    "\n",
    "print(\"\\n3. üîç Anomaly Detection:\")\n",
    "print(\"   - Outliers easier to spot in 2D/3D\")\n",
    "print(\"   - Large distances in reduced space = anomalies\")\n",
    "print(\"   - Useful for fraud detection, quality control\")\n",
    "\n",
    "print(\"\\n4. üéØ Feature Engineering:\")\n",
    "print(\"   - Principal components become new features\")\n",
    "print(\"   - Often better than original features\")\n",
    "print(\"   - Removes multicollinearity issues\")\n",
    "\n",
    "print(\"\\n5. üìà Exploratory Data Analysis:\")\n",
    "print(\"   - Understand data structure quickly\")\n",
    "print(\"   - Identify natural clusters\")\n",
    "print(\"   - Guide further analysis\")\n",
    "\n",
    "# Example: Find potential outliers using PCA\n",
    "print(f\"\\nüéØ EXAMPLE: OUTLIER DETECTION\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Calculate distance from origin in PCA space\n",
    "pca_distances = np.sqrt(X_pca_2d[:, 0]**2 + X_pca_2d[:, 1]**2)\n",
    "outlier_threshold = np.percentile(pca_distances, 95)  # Top 5% as outliers\n",
    "\n",
    "potential_outliers = np.where(pca_distances > outlier_threshold)[0]\n",
    "print(f\"Found {len(potential_outliers)} potential outliers (top 5% by PCA distance)\")\n",
    "\n",
    "if len(potential_outliers) > 0:\n",
    "    print(f\"Example outlier indices: {potential_outliers[:5]}\")\n",
    "    \n",
    "    # Show outliers on plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], alpha=0.6, s=20, label='Normal points')\n",
    "    plt.scatter(X_pca_2d[potential_outliers, 0], X_pca_2d[potential_outliers, 1], \n",
    "                color='red', s=50, alpha=0.8, label='Potential outliers')\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.title('Outlier Detection using PCA')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecfbb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare clustering results with and without dimensionality reduction\n",
    "print(\"üî¨ DIMENSIONALITY REDUCTION FOR CLUSTERING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Original high-dimensional clustering\n",
    "print(\"Clustering in original space...\")\n",
    "kmeans_original = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "labels_original = kmeans_original.fit_predict(X_scaled)\n",
    "silhouette_original = silhouette_score(X_scaled, labels_original)\n",
    "\n",
    "# PCA-reduced clustering  \n",
    "print(\"Clustering in PCA space...\")\n",
    "kmeans_pca = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "labels_pca = kmeans_pca.fit_predict(X_pca_optimal)\n",
    "silhouette_pca = silhouette_score(X_pca_optimal, labels_pca)\n",
    "\n",
    "# 2D visualisation of clustering results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Original space clusters (projected to 2D PCA for visualisation)\n",
    "scatter1 = axes[0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], \n",
    "                          c=labels_original, cmap='viridis', alpha=0.6, s=20)\n",
    "axes[0].set_xlabel('First Principal Component')\n",
    "axes[0].set_ylabel('Second Principal Component') \n",
    "axes[0].set_title(f'Clustering in Original Space\\n(Silhouette: {silhouette_original:.3f})')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# PCA space clusters\n",
    "scatter2 = axes[1].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], \n",
    "                          c=labels_pca, cmap='viridis', alpha=0.6, s=20)\n",
    "axes[1].set_xlabel('First Principal Component')\n",
    "axes[1].set_ylabel('Second Principal Component')\n",
    "axes[1].set_title(f'Clustering in PCA Space\\n(Silhouette: {silhouette_pca:.3f})')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä Clustering Comparison:\")\n",
    "print(f\"- Original space ({X_scaled.shape[1]}D): Silhouette = {silhouette_original:.3f}\")\n",
    "print(f\"- PCA space ({components_90}D): Silhouette = {silhouette_pca:.3f}\")\n",
    "\n",
    "if silhouette_pca > silhouette_original:\n",
    "    print(\"‚úì PCA improved clustering quality!\")\n",
    "    print(\"  Benefits: Faster computation + better separation\")\n",
    "else:\n",
    "    print(\"‚Ä¢ Original space performed better\")\n",
    "    print(\"  This suggests important information in discarded components\")\n",
    "\n",
    "print(f\"\\nüí° Computational Benefits:\")\n",
    "print(f\"- Original: {X_scaled.shape[1]} features √ó {len(X_scaled)} points = {X_scaled.shape[1] * len(X_scaled):,} values\")\n",
    "print(f\"- PCA: {components_90} features √ó {len(X_scaled)} points = {components_90 * len(X_scaled):,} values\")\n",
    "print(f\"- Reduction: {(1 - (components_90 * len(X_scaled)) / (X_scaled.shape[1] * len(X_scaled)))*100:.1f}% fewer values!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923c0e23",
   "metadata": {},
   "source": [
    "## 8. When to Use PCA vs UMAP\n",
    "\n",
    "Understanding when to use each technique is crucial for effective analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b6bde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guidelines for choosing between PCA and UMAP\n",
    "print(\"üéØ CHOOSING BETWEEN PCA AND UMAP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"üî¨ Use PCA when:\")\n",
    "print(\"‚Ä¢ You want to understand global data structure\")\n",
    "print(\"‚Ä¢ You need interpretable components (feature importance)\")\n",
    "print(\"‚Ä¢ You want to reduce data size for machine learning\")\n",
    "print(\"‚Ä¢ You have linear relationships in your data\")\n",
    "print(\"‚Ä¢ You need fast, deterministic results\")\n",
    "print(\"‚Ä¢ You want to reverse the transformation later\")\n",
    "\n",
    "print(f\"\\nüó∫Ô∏è Use UMAP when:\")\n",
    "print(\"‚Ä¢ You want to visualise complex, non-linear data\")\n",
    "print(\"‚Ä¢ You care more about local structure than global\")\n",
    "print(\"‚Ä¢ You want to find clusters and neighbourhoods\")\n",
    "print(\"‚Ä¢ Your data has manifold structure\")\n",
    "print(\"‚Ä¢ You're doing exploratory data analysis\")\n",
    "print(\"‚Ä¢ You can accept some randomness in results\")\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è Comparison Summary:\")\n",
    "comparison_table = \"\"\"\n",
    "Aspect              | PCA                    | UMAP\n",
    "--------------------|------------------------|-------------------------\n",
    "Speed               | Very Fast              | Slower (especially large data)\n",
    "Interpretability    | High (loadings)        | Low (no feature weights)\n",
    "Deterministic       | Yes                    | No (random seed dependent)\n",
    "Linear/Non-linear   | Linear only            | Non-linear\n",
    "Preserves           | Global structure       | Local structure\n",
    "Good for ML         | Yes                    | Sometimes\n",
    "Good for visualisation | Moderate            | Excellent\n",
    "Memory usage        | Low                    | Higher\n",
    "\"\"\"\n",
    "print(comparison_table)\n",
    "\n",
    "print(f\"\\nüèÜ Best Practice Workflow:\")\n",
    "print(\"1. Start with PCA to understand global structure\")\n",
    "print(\"2. Use PCA for dimensionality reduction in ML pipelines\")\n",
    "print(\"3. Use UMAP for detailed visualisation and exploration\")\n",
    "print(\"4. Compare both results to get complete picture\")\n",
    "print(\"5. Choose based on your specific use case\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ddd3d3",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "**üéâ Congratulations! You've mastered dimensionality reduction techniques!**\n",
    "\n",
    "**What we learned:**\n",
    "- How to prepare high-dimensional data for analysis\n",
    "- Principal Component Analysis (PCA) for finding main patterns\n",
    "- UMAP for preserving local structure and creating beautiful visualisations\n",
    "- How to interpret and analyse the results\n",
    "- Practical applications in machine learning and data analysis\n",
    "- When to use each technique\n",
    "\n",
    "**Key takeaways:**\n",
    "- **PCA**: Best for understanding global structure, feature importance, and ML preprocessing\n",
    "- **UMAP**: Excellent for visualisation, finding clusters, and exploring local patterns\n",
    "- **Both** provide different but complementary views of your data\n",
    "- **Standardisation** is crucial for both techniques\n",
    "- **Dimensionality reduction** makes complex data understandable and visualisable\n",
    "\n",
    "**Try this next:**\n",
    "1. Apply these techniques to different datasets\n",
    "2. Experiment with different numbers of PCA components\n",
    "3. Try different UMAP parameters (n_neighbors, min_dist)\n",
    "4. Combine with clustering analysis from the previous notebook\n",
    "5. Use reduced dimensions as features for supervised learning\n",
    "6. Explore t-SNE as another non-linear technique\n",
    "\n",
    "**Advanced applications:**\n",
    "- Feature selection based on PCA loadings\n",
    "- Anomaly detection using reconstruction error\n",
    "- Data compression and noise reduction\n",
    "- Creating interpretable machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cea9d10",
   "metadata": {},
   "source": [
    "### üí° Troubleshooting Tips\n",
    "\n",
    "**Common issues and solutions:**\n",
    "\n",
    "1. **PCA shows low explained variance:**\n",
    "   - Your data might have many important dimensions\n",
    "   - Consider keeping more components\n",
    "   - Check if features are properly standardised\n",
    "\n",
    "2. **UMAP results look strange:**\n",
    "   - Try different n_neighbors (5-50)\n",
    "   - Adjust min_dist (0.01-0.5)\n",
    "   - Ensure data is standardised\n",
    "   - Use different random_state values\n",
    "\n",
    "3. **Very slow UMAP computation:**\n",
    "   - Sample your data (use 10,000 points maximum)\n",
    "   - Reduce number of features first with PCA\n",
    "   - Consider using approximate algorithms\n",
    "\n",
    "4. **Interpretation difficulties:**\n",
    "   - Use PCA loadings to understand components\n",
    "   - Color visualisations by known categories\n",
    "   - Compare multiple random seeds for UMAP\n",
    "   - Cross-reference with domain knowledge\n",
    "\n",
    "5. **Poor clustering after dimensionality reduction:**\n",
    "   - Important information might be in discarded components\n",
    "   - Try keeping more PCA components\n",
    "   - Consider using UMAP coordinates for clustering\n",
    "   - Compare results with original high-dimensional space\n",
    "\n",
    "**Remember**: Dimensionality reduction is an art as much as a science - experiment and compare different approaches!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sc2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
